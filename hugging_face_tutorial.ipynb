{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face Transformers Tutorial\n",
    "\n",
    "### Introduction\n",
    "* This notebook provides a comprehensive guide to using the Hugging Face `transformers` library for NLP tasks.\n",
    "\n",
    "* Hugging Face's `transformers` offers easy access to a range of pre-trained models and tools for NLP tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Pipelines\n",
    "* Hugging Face pipelines simplify common NLP tasks like text generation, sentiment analysis, and named entity recognition (NER).\n",
    "\n",
    "* We'll start by using pipelines for quick NLP tasks.\n",
    "\n",
    "#### Sentiment Analysis Pipeline\n",
    "* Let's try the sentiment analysis pipeline to analyze the sentiment of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis: [{'label': 'POSITIVE', 'score': 0.9998641014099121}]\n"
     ]
    }
   ],
   "source": [
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "result = sentiment_pipeline(\"I love Hugging Face!\")\n",
    "print(\"Sentiment Analysis:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Generation Pipeline\n",
    "* We can use the text generation pipeline with models like GPT-2 for generating text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text: Once upon a time, the gods had been more like the good and the evil.\n",
      "\n",
      "The ancient world was full of these beings.\n",
      "\n",
      "And yet their deeds were terrible, for the gods were a thing of life.\n",
      "\n",
      "This is\n"
     ]
    }
   ],
   "source": [
    "text_gen_pipeline = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "generated_text = text_gen_pipeline(\"Once upon a time\", max_length=50, num_return_sequences=1)\n",
    "print(\"\\nGenerated Text:\", generated_text[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named Entity Recognition (NER) Pipeline\n",
    "* The NER pipeline identifies entities like persons, organizations, and locations in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Named Entities: [{'entity_group': 'ORG', 'score': 0.97738045, 'word': 'Hugging Face', 'start': 0, 'end': 12}, {'entity_group': 'LOC', 'score': 0.9990541, 'word': 'New York', 'start': 25, 'end': 33}, {'entity_group': 'ORG', 'score': 0.9995321, 'word': 'Microsoft', 'start': 61, 'end': 70}]\n"
     ]
    }
   ],
   "source": [
    "ner_pipeline = pipeline(\"ner\", grouped_entities=True)\n",
    "entities = ner_pipeline(\"Hugging Face is based in New York and has a partnership with Microsoft.\")\n",
    "print(\"\\nNamed Entities:\", entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question Answering Pipeline\n",
    "* The question-answering pipeline can answer questions based on a given context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question Answering: {'score': 0.9733557105064392, 'start': 40, 'end': 48, 'answer': 'New York'}\n"
     ]
    }
   ],
   "source": [
    "qa_pipeline = pipeline(\"question-answering\")\n",
    "context = \"Hugging Face Inc. is a company based in New York.\"\n",
    "question = \"Where is Hugging Face based?\"\n",
    "answer = qa_pipeline(question=question, context=context)\n",
    "print(\"\\nQuestion Answering:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Pre-trained Models and Tokenizers\n",
    "* Hugging Face provides various models for different tasks, including `bert-base-uncased`, `distilbert-base-uncased`, etc.\n",
    "\n",
    "* Here, we'll load a `distilbert` model and tokenizer for sequence classification.\n",
    "\n",
    "#### Load Model and Tokenizer\n",
    "* We'll use the `distilbert-base-uncased` model for binary sentiment classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized Input: {'input_ids': tensor([[  101,  1045,  2293,  2478, 17662,  2227, 19081,  3075,   999,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "\n",
      "Predicted Class (0 = Negative, 1 = Positive): 1\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Tokenization\n",
    "# Tokenize input text for the model.\n",
    "\n",
    "text = \"I love using Hugging Face transformers library!\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "print(\"\\nTokenized Input:\", inputs)\n",
    "\n",
    "# Model Inference\n",
    "# Run inference to get sentiment predictions.\n",
    "\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "predicted_class = torch.argmax(logits).item()\n",
    "print(\"\\nPredicted Class (0 = Negative, 1 = Positive):\", predicted_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuning a Model on Custom Dataset\n",
    "* We'll fine-tune a `distilbert` model on a custom dataset from Hugging Face's `datasets` library.\n",
    "\n",
    "#### Load Dataset\n",
    "* Load the IMDB dataset for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d81192bfca241a0bcc29f8eba949040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"imdb\")\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]\n",
    "\n",
    "# Preprocess Dataset\n",
    "# Tokenize and format the data for PyTorch.\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "train_data = train_data.map(tokenize_function, batched=True)\n",
    "test_data = test_data.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set format to PyTorch tensors\n",
    "train_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_data.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# Define Trainer\n",
    "# Use `Trainer` for simplified model training.\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcabcf213eb14d24aaea546ec308ef5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3913, 'grad_norm': 14.22544002532959, 'learning_rate': 1.6800000000000002e-05, 'epoch': 0.16}\n",
      "{'loss': 0.3578, 'grad_norm': 11.48914623260498, 'learning_rate': 1.3600000000000002e-05, 'epoch': 0.32}\n",
      "{'loss': 0.3499, 'grad_norm': 7.203550338745117, 'learning_rate': 1.04e-05, 'epoch': 0.48}\n",
      "{'loss': 0.3403, 'grad_norm': 7.7178473472595215, 'learning_rate': 7.2000000000000005e-06, 'epoch': 0.64}\n",
      "{'loss': 0.3246, 'grad_norm': 14.340924263000488, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.8}\n",
      "{'loss': 0.3134, 'grad_norm': 20.36943244934082, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.96}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b85d382f605b4bccb4ca80359bb40ec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.31017592549324036, 'eval_runtime': 281.1914, 'eval_samples_per_second': 88.907, 'eval_steps_per_second': 11.113, 'epoch': 1.0}\n",
      "{'train_runtime': 1273.1583, 'train_samples_per_second': 19.636, 'train_steps_per_second': 2.455, 'train_loss': 0.34508411376953124, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3125, training_loss=0.34508411376953124, metrics={'train_runtime': 1273.1583, 'train_samples_per_second': 19.636, 'train_steps_per_second': 2.455, 'total_flos': 827921241600000.0, 'train_loss': 0.34508411376953124, 'epoch': 1.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# May take up to 20 minutes or more due to CPU use\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Model\n",
    "* Evaluate the fine-tuned model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de20691e88e24100bd237e58c07abb28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results: {'eval_loss': 0.31017592549324036, 'eval_runtime': 255.9775, 'eval_samples_per_second': 97.665, 'eval_steps_per_second': 12.208, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "evaluation_results = trainer.evaluate()\n",
    "print(\"\\nEvaluation Results:\", evaluation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save and Load Model\n",
    "* Save the fine-tuned model and tokenizer to a local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "# Load the model back\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(\"./fine_tuned_model\")\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(\"./fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference with Fine-tuned Model\n",
    "* Perform inference with the newly fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fine-tuned Model Prediction (0 = Negative, 1 = Positive): 1\n"
     ]
    }
   ],
   "source": [
    "text = \"The movie was fantastic!\"\n",
    "inputs = loaded_tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = loaded_model(**inputs)\n",
    "logits = outputs.logits\n",
    "predicted_class = torch.argmax(logits).item()\n",
    "print(\"\\nFine-tuned Model Prediction (0 = Negative, 1 = Positive):\", predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
