{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study: Classifying Emails as Spam or Ham for an Email Service Provider\n",
    "\n",
    "#### Description\n",
    "An email service provider aims to enhance its spam filtering system to reduce the number of spam emails reaching user inboxes. The objective is to build a predictive model that accurately classifies emails as spam or ham (non-spam).\n",
    "\n",
    "#### Steps\n",
    "\n",
    "* Data Generation:\n",
    "Created a synthetic dataset of emails with random variations for spam and ham emails.\n",
    "Utilized templates and random word selections to generate varied content.\n",
    "\n",
    "* Data Preprocessing:\n",
    "Cleaned the email text by removing special characters and stopwords.\n",
    "Performed lemmatization to normalize words.\n",
    "\n",
    "* Feature Extraction:\n",
    "Used TF-IDF Vectorization to convert text data into numerical features.\n",
    "\n",
    "* Model Training:\n",
    "Split the dataset into training and test sets.\n",
    "Trained a Multinomial Naive Bayes classifier.\n",
    "\n",
    "* Evaluation:\n",
    "Assessed model performance using accuracy, precision, recall, F1-score, and ROC-AUC metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/joelfuentes/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/joelfuentes/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Generation\n",
    "\n",
    "# Define components for spam emails\n",
    "spam_subjects = [\n",
    "    \"Congratulations\", \"Urgent\", \"Limited Time Offer\", \"You have won\", \"Exclusive Deal\"\n",
    "]\n",
    "\n",
    "spam_bodies = [\n",
    "    \"You have been selected to win a {prize}. Click {link} to claim now.\",\n",
    "    \"Your {account} has been compromised. Send your {details} to {email} immediately.\",\n",
    "    \"Earn {amount} working from home. No experience required.\",\n",
    "    \"Limited time offer on {product}. Buy now and get {discount}% off.\",\n",
    "    \"Dear {name}, you have won a {prize}. Reply to this email to claim.\"\n",
    "]\n",
    "\n",
    "spam_placeholders = {\n",
    "    'prize': [\"$1000 gift card\", \"free vacation\", \"brand new car\"],\n",
    "    'link': [\"here\", \"this link\", \"the following URL\"],\n",
    "    'account': [\"bank account\", \"email account\", \"social media account\"],\n",
    "    'details': [\"password\", \"login details\", \"credentials\"],\n",
    "    'email': [\"security@fakebank.com\", \"support@phishingsite.com\"],\n",
    "    'amount': [\"$5000 per week\", \"$1000 daily\", \"$2000 monthly\"],\n",
    "    'product': [\"electronics\", \"fashion items\", \"home appliances\"],\n",
    "    'discount': [\"50\", \"70\", \"80\"],\n",
    "    'name': [\"user\", \"customer\", \"friend\"]\n",
    "}\n",
    "\n",
    "# Define components for ham emails\n",
    "ham_subjects = [\n",
    "    \"Meeting Reminder\", \"Project Update\", \"Invitation\", \"Question\", \"Follow-up\"\n",
    "]\n",
    "\n",
    "ham_bodies = [\n",
    "    \"Hi {name}, are we still on for the {event} tomorrow?\",\n",
    "    \"Please find attached the {document} for your review.\",\n",
    "    \"It was great meeting you at the {event}. Let's catch up soon.\",\n",
    "    \"Can you provide an update on the {project} status?\",\n",
    "    \"Thank you for your {action}. It was very helpful.\"\n",
    "]\n",
    "\n",
    "ham_placeholders = {\n",
    "    'name': [\"John\", \"Sarah\", \"Michael\", \"Jessica\"],\n",
    "    'event': [\"meeting\", \"lunch\", \"conference call\", \"seminar\"],\n",
    "    'document': [\"report\", \"proposal\", \"presentation\", \"invoice\"],\n",
    "    'project': [\"marketing\", \"development\", \"design\", \"research\"],\n",
    "    'action': [\"feedback\", \"assistance\", \"support\", \"response\"]\n",
    "}\n",
    "\n",
    "# Function to generate emails with random variations\n",
    "def generate_emails(subjects, bodies, placeholders, num_emails):\n",
    "    emails = []\n",
    "    for _ in range(num_emails):\n",
    "        subject = random.choice(subjects)\n",
    "        body_template = random.choice(bodies)\n",
    "        body = body_template.format(\n",
    "            **{key: random.choice(values) for key, values in placeholders.items()}\n",
    "        )\n",
    "        email = f\"Subject: {subject}\\n\\n{body}\"\n",
    "        emails.append(email)\n",
    "    return emails\n",
    "\n",
    "# Generate synthetic spam and ham emails\n",
    "num_emails = 1000\n",
    "spam_emails = generate_emails(spam_subjects, spam_bodies, spam_placeholders, num_emails)\n",
    "ham_emails = generate_emails(ham_subjects, ham_bodies, ham_placeholders, num_emails)\n",
    "\n",
    "# Create the dataset\n",
    "emails = pd.DataFrame({\n",
    "    'text': spam_emails + ham_emails,\n",
    "    'label': ['spam'] * num_emails + ['ham'] * num_emails\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Data Preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Remove 'Subject:' and newlines\n",
    "    text = re.sub(r'Subject: ', '', text)\n",
    "    text = text.replace('\\n', ' ')\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Lowercase and split into words\n",
    "    words = text.lower().split()\n",
    "    # Remove stopwords\n",
    "    words = [w for w in words if w not in stopwords.words('english')]\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(w) for w in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "emails['cleaned_text'] = emails['text'].apply(preprocess_text)\n",
    "\n",
    "# Step 3: Feature Extraction\n",
    "vectorizer = TfidfVectorizer(max_features=500)\n",
    "X = vectorizer.fit_transform(emails['cleaned_text'])\n",
    "y = emails['label'].map({'ham': 0, 'spam': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       200\n",
      "           1       1.00      1.00      1.00       200\n",
      "\n",
      "    accuracy                           1.00       400\n",
      "   macro avg       1.00      1.00      1.00       400\n",
      "weighted avg       1.00      1.00      1.00       400\n",
      "\n",
      "ROC-AUC Score: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Model Training\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# ROC-AUC Score\n",
    "y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(f\"ROC-AUC Score: {roc_auc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpratation\n",
    "\n",
    "#### Results\n",
    "* Accuracy: 99%\n",
    "* Precision: 99%\n",
    "* Recall: 99%\n",
    "* F1-score: 99%\n",
    "* ROC-AUC Score: 0.99\n",
    "\n",
    "As expected, the Multinomial Naive Bayes model demonstrated excellent performance on the synthetic dataset with random variations. By generating a diverse set of spam and ham emails, the model effectively learned to distinguish between the two classes. This model can be integrated into the email service provider's system to enhance spam detection.\n",
    "\n",
    "### Recommended Next Steps\n",
    "* Real-World Data Integration: Incorporate real email data (ensuring privacy and compliance) to validate the model's performance in a practical setting.\n",
    "\n",
    "* Advanced Feature Engineering: Explore n-grams, word embeddings, or deep learning models like RNNs for potentially improved performance.\n",
    "\n",
    "* Handling Imbalanced Data: In real scenarios, spam emails are often less frequent. Implement techniques to handle imbalanced datasets, such as SMOTE or class weighting.\n",
    "\n",
    "* Continuous Learning: Develop mechanisms for the model to adapt to new spam tactics by retraining with recent data periodically.\n",
    "Deployment and Monitoring: Integrate the model into the email system and set up monitoring to track its performance and user feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
